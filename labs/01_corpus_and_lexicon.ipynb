{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Corpus and Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - relation between corpus and lexicon\n",
    "    - effects of pre-processing (tokenization) on lexicon\n",
    "    \n",
    "- Learning how to:\n",
    "    - load basic corpora for processing\n",
    "    - compute basic descriptive statistic of a corpus\n",
    "    - building lexicon and frequency lists from a corpus\n",
    "    - perform basic lexicon operations\n",
    "    - perform basic text pre-processing (tokenization and sentence segmentation) using python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 2: Regular Expressions, Text Normalization, Edit Distance](https://web.stanford.edu/~jurafsky/slp3/2.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 2: Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)\n",
    "    - [Chapter 3: Processing Raw Text](https://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "    \n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English language model (`spacy>=3.0`)\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "    - run `pip install scikit-learn`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Text Corpora in NLTK\n",
    "NLTK provides several corpora with loading functions. Plain text corpora come from a _Project Gutenberg_.\n",
    "\n",
    "`nltk.corpus.gutenberg.fileids()` lists available books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\xdieg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xdieg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Units of Text Corpus\n",
    "Depending on a goal, corpus can be seen as a sequence of:\n",
    "- characters\n",
    "- words (tokens)\n",
    "- sentences\n",
    "- paragraphs\n",
    "- document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each level, in turn, can be seen as a sequence of elements of the previous level.\n",
    "\n",
    "- word -- a sequence of characters\n",
    "- sentence -- a sequence of words\n",
    "- paragraph -- a sequence of sentences\n",
    "- document -- a sequence of paragraphs (or sentences, depending on our purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Loading NLTK Corpora\n",
    "\n",
    "NLTK provides functions to load a corpus using these different levels, as `raw` (characters), `words`, and `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: [\n",
      "words: [\n",
      "sents: ['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']\n"
     ]
    }
   ],
   "source": [
    "alice_chars = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "print('chars:', alice_chars[0])\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "print('words:', alice_words[0])\n",
    "alice_sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "print('sents:', alice_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* can be described in terms of:\n",
    "\n",
    "- total number of characters\n",
    "- total number of words (_tokens_: includes punctuation, etc.)\n",
    "- total number of sentences\n",
    "\n",
    "- minimum/maximum/average number of character per token\n",
    "- minimum/maximum/average number of words per sentence\n",
    "- minimum/maximum/average number of sentences per document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example__\n",
    "\n",
    "$$\\text{Av. Token Count} = \\frac{\\text{count}(tokens)}{\\text{count}(sentences)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's compute average sentence length & round to closes integer\n",
    "round(len(alice_words)/len(alice_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG sent len 20\n",
      "MIN sent len 2\n",
      "MAX sent len 204\n"
     ]
    }
   ],
   "source": [
    "# let's compute length of each sentence\n",
    "sent_lens = [len(sent) for sent in alice_sents]\n",
    "# let's compute length of each word\n",
    "word_lens = [len(word) for word in alice_words]\n",
    "# let's compute length the number of characters in each sentence\n",
    "chars_lens = [len(''.join(sent)) for sent in alice_sents]\n",
    "\n",
    "avg_sent_len = round(sum(sent_lens)/len(sent_lens))\n",
    "min_sent_len = min(sent_lens)\n",
    "max_sent_len = max(sent_lens)\n",
    "print(\"AVG sent len\", avg_sent_len)\n",
    "print(\"MIN sent len\", min_sent_len)\n",
    "print(\"MAX sent len\", max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "H⭐e⭐l⭐l⭐o\n"
     ]
    }
   ],
   "source": [
    "# JOIN built-in function example\n",
    "tmp = ['H', 'e', 'l', 'l', 'o']\n",
    "print(''.join(tmp))\n",
    "print('⭐'.join(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- Define a function to compute corpus descriptive statistics\n",
    "\n",
    "    - input:\n",
    "        - raw text (Chars)\n",
    "        - words\n",
    "        - sentences\n",
    "    - output (print): \n",
    "        - average number of:\n",
    "            - chars per word\n",
    "            - words per sentence\n",
    "            - chars per sentence\n",
    "        - Size of the longest word and sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word per sentence 20\n",
      "Char per word 3\n",
      "Char per sentence 68\n",
      "Longest sentence 204\n",
      "Longest word 14\n"
     ]
    }
   ],
   "source": [
    "def statistics(chars, words, sents):\n",
    "    word_lens = [len(w) for w in words] # Add word lens\n",
    "    sent_lens = [len(s) for s in sents] # Add sentence lens\n",
    "    chars_in_sents = [len(\"\".join(s)) for s in sents] # Add char lens\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sent_lens))\n",
    "    char_per_word = round(sum(word_lens) / len(word_lens))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(chars_in_sents))\n",
    "    \n",
    "    longest_sentence = max(sent_lens)\n",
    "    longest_word = max(word_lens)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word = statistics(alice_chars, alice_words, alice_sents)\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word)\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sent)\n",
    "print('Longest word', longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Lexicon and Its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Constructing Lexicon and Computing its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lexicon is a list of unique elemets, it is a `set` of corpus words (i.e. tokens).\n",
    "Consequently, its size is the size of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set(alice_words)\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:\n",
    "We did not process our corpus in any way. Consequently, words with case variations are different entries in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Lowercased Lexicon\n",
    "Let's lowercase our corpus and re-compute the lexicon size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2636"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set([w.lower() for w in alice_words])\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Computing Frequency List with python\n",
    "\n",
    "In python, frequency list can be constructed in several ways. The most convenient is the `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "alice_freq_list = Counter(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_list.get('ALL', 0))\n",
    "print(alice_freq_list.get('All', 0))\n",
    "print(alice_freq_list.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Computing Frequency List with NLTK\n",
    "NLTK provides `FreqDist` class to construct a Frequency List (`FreqDist` == _Frequency Distribution_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_freq_dist = nltk.FreqDist(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_dist.get('ALL', 0))\n",
    "print(alice_freq_dist.get('All', 0))\n",
    "print(alice_freq_dist.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- compute frequency list of __lowercased__ \"alice\" corpus (you can use either method)\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compare the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| ,      |     1,993 |\n",
    "| '      |     1,731 |\n",
    "| the    |     1,642 |\n",
    "| and    |       872 |\n",
    "| .      |       764 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 1993, \"'\": 1731, 'the': 1642, 'and': 872, '.': 764}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lowercase_freq_list = Counter([w.lower() for w in alice_words]) # Replace X with the word list of the corpus in lower case (see above))\n",
    "nbest(alice_lowercase_freq_list, n=5) # Change N form 1 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.3.1. Frequency Cut-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "<!-- - define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "    \n",
    "    - input: frequence list (dict)\n",
    "    - output: list\n",
    "    - use default values for min and max\n",
    "     -->\n",
    "- Using the function cut_off\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table (on the lowercased lexicon)\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 2636 |\n",
    "| cut-off    |   2 | N/A | 1503 |\n",
    "| cut-off    | N/A | 100 | 2586 |\n",
    "| cut-off    |   2 | 100 | 1453 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "CutOFF Min: 2.0 MAX: 100.0  Lexicon Size: 1453\n"
     ]
    }
   ],
   "source": [
    "def cut_off(vocab, n_min=100, n_max=100):\n",
    "    new_vocab = []\n",
    "    # frequency data structure is a dictionary\n",
    "    for word, count in vocab.items():\n",
    "        if count >= n_min and count <= n_max:\n",
    "            new_vocab.append(word)\n",
    "    return new_vocab\n",
    "\n",
    "lower_bound = float(\"2\") # Change these two number to compute the required cut offs\n",
    "upper_bound = float(\"100\")\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. StopWord Removal\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's check the stop word lists from the popular python libraries.\n",
    "\n",
    "- spaCy\n",
    "- NLTK\n",
    "- scikit-learn\n",
    "\n",
    "    \n",
    "For NLTK we need to download them first\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xdieg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 326\n",
      "NLTK: 179\n",
      "sklearn: 318\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
    "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
    "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    - compute the intersection between the 100 most frequent words in frequency list of the alice corpus and the list of stopwords (report count)\n",
    "    - remove stopwords from the lexicon\n",
    "    - print the size of:\n",
    "            - original lexicon\n",
    "            - lexicon without stopwords\n",
    "            - overlap between 100 most freq. words and stopwords\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 2636 |\n",
    "| no stop words   | 2490 |\n",
    "| top 100 overlap |   65 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'a'}\n",
      "{'c', 'd', 'e'}\n"
     ]
    }
   ],
   "source": [
    "# Set built-in Function\n",
    "set_a = set(['a', 'b', 'c', 'd', 'e'])\n",
    "set_b = set(['a', 'b', 'f'])\n",
    "\n",
    "print(set_a.intersection(set_b)) # Compute overlap\n",
    "print(set_a.difference(set_b)) # Remove Elements by computing the set diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "No stopwords 2571\n",
      "Top100 overlap 65\n"
     ]
    }
   ],
   "source": [
    "alice_vocab = set([w.lower() for w in alice_words])\n",
    "top100 = list(nbest(alice_lowercase_freq_list, n=100).keys())\n",
    "\n",
    "stop_words = NLTK_STOP_WORDS\n",
    "\n",
    "# semantically, overlap means tokens present also in top100 (most frequent tokens in alice vocab) that are stopwords indeed.\n",
    "overlap = set(top100).intersection(set(stop_words)) # Compute the intersation between top100 and stop_words\n",
    "alice_vocab_no_stopwords = alice_vocab.difference(overlap) # Remove Stopwords from alice vocab\n",
    "\n",
    "print('Original', len(alice_vocab))\n",
    "print('No stopwords', len(alice_vocab_no_stopwords))\n",
    "print('Top100 overlap', len(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Basic Text Pre-processing\n",
    "\n",
    "Both frequency cut-off and stop word removal are frequently used text pre-processing steps. Depending on the application, there are several other common text pre-processing steps that are usually applied for tranforming text for Machine Learning tasks.\n",
    "\n",
    "__Text Normalization Steps__\n",
    "\n",
    "- removing extra white spaces\n",
    "\n",
    "- tokenization\n",
    "    - documents to sentences (sentence segmentation/tokenization)\n",
    "    - sentences to tokens\n",
    "\n",
    "- lowercasing/uppercasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- removing punctuation\n",
    "\n",
    "- removing accent marks and other diacritics \n",
    "\n",
    "- removing stop words (see above)\n",
    "\n",
    "- removing sparse terms (frequency cut-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- number normalization\n",
    "    - numbers to words (i.e. `10` to `ten`)\n",
    "    - number words to numbers (i.e. `ten` to `10`)\n",
    "    - removing numbers\n",
    "\n",
    "- verbalization (specifically for speech applications)\n",
    "\n",
    "    - numbers to words\n",
    "    - expanding abbreviations (or spelling out)\n",
    "    - reading out dates, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    - the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "- [stemming](https://en.wikipedia.org/wiki/Stemming)\n",
    "    - the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenization and Sentence Segmentation\n",
    "\n",
    "Given a \"clean\" text, in order to allow any analysis, we need to identify its units.\n",
    "In other words, we need to _segment_ the text into sentences and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:\n",
    "Since both _tokenization_ and _sentence segmentation_ are automatic, different tools yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Tokenization and Sentence Segmentation with spaCy\n",
    "The default spaCy NLP pipeline does several processing steps including __tokenization__, *part of speech tagging*, lemmatization, *dependency parsing* and *Named Entity Recognition* (ignore the ones in *italics* for today). \n",
    "\n",
    "\n",
    "SpaCy produces a `Doc` object that contains `Span`s (sentences) and `Token`s (words).\n",
    "\n",
    "*Installing the model*\n",
    "```python\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "# un-comment the lines above, if you get 'ModuleNotFoundError'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txt = alice_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xdieg\\anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# process the document\n",
    "doc = nlp(txt, disable=[\"tagger\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: '['\n",
      "first sentence: '[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "print(\"first token: '{}'\".format(doc[0]))\n",
    "print(\"first sentence: '{}'\".format(list(doc.sents)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37033\n",
      "1558\n"
     ]
    }
   ],
   "source": [
    "# access list of tokens (Token objects)\n",
    "print(len(doc))\n",
    "# access list of sentences (Span objects)\n",
    "print(len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Tokenization and Sentence Segmentation with NLTK\n",
    "NLTK's [tokenize](https://www.nltk.org/api/nltk.tokenize.html) package provides similar functionality using the methods below.\n",
    "\n",
    "- `word_tokenize` \n",
    "- `sent_tokenize`\n",
    "\n",
    "There are several tokenizer available (read documentation for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xdieg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33494\n",
      "1625\n"
     ]
    }
   ],
   "source": [
    "alice_words_nltk = nltk.word_tokenize(alice_chars) # alice_chars is a book string\n",
    "alice_sents_nltk = nltk.sent_tokenize(alice_chars)\n",
    "print(len(alice_words_nltk))\n",
    "print(len(alice_sents_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: '['\n",
      "first sentence: '[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.'\n"
     ]
    }
   ],
   "source": [
    "print(\"first token: '{}'\".format(alice_words_nltk[0]))\n",
    "print(\"first sentence: '{}'\".format(alice_sents_nltk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab Exercise\n",
    "- Load another corpus from Gutenberg (e.g. `milton-paradise.txt`)\n",
    "- Compute descriptive statistics on the __reference__ sentences and tokens.\n",
    "- Compute descriptive statistics in the __automatically__ processed corpus\n",
    "    - both with `spacy` and `nltk`\n",
    "- Compute lowercased lexicons for all 3 versions of the corpus\n",
    "    - compare lexicon sizes\n",
    "- Compute frequency distribution for all 3 versions of the corpus\n",
    "    - compare top N frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "milton_chars = nltk.corpus.gutenberg.raw('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive statistics on reference sentences and tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word per sentence:\t 52\n",
      "Char per word:\t\t 5\n",
      "Char per sentence:\t 203\n",
      "Longest sentence len:\t 533\n",
      "Longest word len:\t 16\n"
     ]
    }
   ],
   "source": [
    "milton_words = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "milton_sents = nltk.corpus.gutenberg.sents('milton-paradise.txt') # joined sentences =/= whole raw text\n",
    "\n",
    "# Or: word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word = statistics(milton_chars, milton_words, milton_sents)\n",
    "\n",
    "word_lens = [len(w) for w in milton_words] # chars in words\n",
    "sents_lens = [len(s) for s in milton_sents] # words in sents\n",
    "chars_sents_lens = [len(\"\".join(s)) for s in milton_sents] # chars in sents\n",
    "\n",
    "word_per_sent = round(sum(sents_lens) / len(sents_lens))\n",
    "char_per_word = round(len(milton_chars) / len(milton_words))\n",
    "char_per_sent = round(sum(chars_sents_lens) / len(milton_sents))\n",
    "longest_sent = max(sents_lens)\n",
    "longest_word = max(word_lens)\n",
    "\n",
    "print('Word per sentence:\\t', word_per_sent)\n",
    "print('Char per word:\\t\\t', char_per_word)\n",
    "print('Char per sentence:\\t', char_per_sent)\n",
    "print('Longest sentence len:\\t', longest_sent)\n",
    "print('Longest word len:\\t', longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reference #corpus words: 96825\n",
      "1. Tokenized #corpus words: 95716\n",
      "\n",
      "2. Reference #corpus sents: 1851\n",
      "2. Tokenized #corpus sents: 1835\n"
     ]
    }
   ],
   "source": [
    "# Comparing reference with automatic fns\n",
    "print(f\"1. Reference #corpus words: {len(milton_words)}\")\n",
    "print(f\"1. Tokenized #corpus words: {len(nltk.word_tokenize(milton_chars))}\\n\")\n",
    "\n",
    "print(f\"2. Reference #corpus sents: {len(milton_sents)}\")\n",
    "print(f\"2. Tokenized #corpus sents: {len(nltk.sent_tokenize(milton_chars))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive statistics on NLTK sentences and tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word per sentence:\t 253\n",
      "Char per word:\t\t 4\n",
      "Char per sentence:\t 253\n",
      "Longest sentence len:\t 2658\n",
      "Longest word len:\t 18\n"
     ]
    }
   ],
   "source": [
    "milton_words_nltk = nltk.word_tokenize(milton_chars)\n",
    "milton_sents_nltk = nltk.sent_tokenize(milton_chars)\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word = statistics(milton_chars, milton_words_nltk, milton_sents_nltk)\n",
    "\n",
    "print('Word per sentence:\\t', word_per_sent)\n",
    "print('Char per word:\\t\\t', char_per_word)\n",
    "print('Char per sentence:\\t', char_per_sent)\n",
    "print('Longest sentence len:\\t', longest_sent)\n",
    "print('Longest word len:\\t', longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK tokenizes sentences in a very different way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Paradise Lost by John Milton 1667] \\n \\n \\nBook I \\n \\n \\nOf Man's first disobedience, and the fruit \\nOf that forbidden tree whose mortal taste \\nBrought death into the World, and all our woe, \\nWith loss of Eden, till one greater Man \\nRestore us, and regain the blissful seat, \\nSing, Heavenly Muse, that, on the secret top \\nOf Oreb, or of Sinai, didst inspire \\nThat shepherd who first taught the chosen seed \\nIn the beginning how the heavens and earth \\nRose out of Chaos: or, if Sion hill \\nDelight thee more, and Siloa's brook that flowed \\nFast by the oracle of God, I thence \\nInvoke thy aid to my adventurous song, \\nThat with no middle flight intends to soar \\nAbove th' Aonian mount, while it pursues \\nThings unattempted yet in prose or rhyme.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milton_sents_nltk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', '1667', ']']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milton_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive statistics on SpaCy sentences and tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word per sentence:\t 48\n",
      "Char per word:\t\t 4\n",
      "Char per sentence:\t 174\n",
      "Longest sentence len:\t 321\n",
      "Longest word len:\t 63\n"
     ]
    }
   ],
   "source": [
    "# Statistics SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(milton_chars, disable=[\"tagger\", \"ner\"])\n",
    "\n",
    "# SpaCy words\n",
    "milton_words_spacy = list(doc)\n",
    "\n",
    "# SpaCy sentences: tokens to strings\n",
    "milton_sents_spacy = []\n",
    "for i, s in enumerate(list(doc.sents)):\n",
    "    sent = [None] * len(s)\n",
    "    for j, w in enumerate(list(s)): \n",
    "        sent[j] = w.orth_\n",
    "    milton_sents_spacy.append(sent)\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word = statistics(milton_chars, milton_words_spacy, milton_sents_spacy)\n",
    "\n",
    "print('Word per sentence:\\t', word_per_sent)\n",
    "print('Char per word:\\t\\t', char_per_word)\n",
    "print('Char per sentence:\\t', char_per_sent)\n",
    "print('Longest sentence len:\\t', longest_sent)\n",
    "print('Longest word len:\\t', longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute lowercased lexicons for all 3 versions of the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference milton-paradise lexicon size: \t9021\n",
      "NLTK milton-paradise lexicon size: \t\t9280\n",
      "SpaCy milton-paradise lexicon size: \t\t9144\n"
     ]
    }
   ],
   "source": [
    "lex_milton = set([w.lower() for w in milton_words])\n",
    "lex_milton_nltk = set([w.lower() for w in milton_words_nltk])\n",
    "lex_milton_spacy = set([w.orth_.lower() for w in milton_words_spacy])\n",
    "\n",
    "print(f\"Reference milton-paradise lexicon size: \\t{len(lex_milton)}\")\n",
    "print(f\"NLTK milton-paradise lexicon size: \\t\\t{len(lex_milton_nltk)}\")\n",
    "print(f\"SpaCy milton-paradise lexicon size: \\t\\t{len(lex_milton_spacy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute frequency distribution for all 3 versions of the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "milton_freqlist = nltk.FreqDist([w.lower() for w in milton_words])\n",
    "milton_freqlist_nltk = nltk.FreqDist([w.lower() for w in milton_words_nltk])\n",
    "milton_freqlist_spacy = nltk.FreqDist([w.orth_.lower() for w in milton_words_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 10198, 'and': 3395, 'the': 2968, ';': 2317, 'to': 2228}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest(milton_freqlist, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 10228, 'and': 3391, 'the': 2964, ';': 2326, 'to': 2223}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest(milton_freqlist_nltk, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 10425, ',': 10224, 'and': 3392, 'the': 2965, ';': 2303}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we notice spacy keeps line breaks!\n",
    "nbest(milton_freqlist_spacy, n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
